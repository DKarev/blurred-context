# -*- coding: utf-8 -*-
"""finetune.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZqbxyGEewWXWeCgslmNRH32iVuv796ka
"""

!pip install pyyaml==5.1 > /dev/null

import torch, torchvision

!python3 -m pip install 'git+https://github.com/facebookresearch/detectron2.git' >/dev/null
!pip install pillow==4.1.1 > /dev/null

# "restart runtime" in Colab after installation!

import os
import cv2
import torch, torchvision
import numpy as np
import json

from collections import OrderedDict

from google.colab import drive
from google.colab.patches import cv2_imshow

import detectron2

from detectron2 import model_zoo
from detectron2.engine import DefaultTrainer
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog, DatasetCatalog

drive.mount("/content/gdrive")
datapath = "/content/gdrive/My Drive/Colab Notebooks/data/unrel-dataset/images"

# load data: COCOstuff

!wget http://images.cocodataset.org/zips/train2017.zip
!unzip train2017.zip > /dev/null
!rm train2017.zip

!wget http://images.cocodataset.org/zips/val2017.zip
!unzip val2017.zip >/dev/null
!rm val2017.zip

!wget http://calvin.inf.ed.ac.uk/wp-content/uploads/data/cocostuffdataset/stuff_trainval2017.zip
!unzip stuff_trainval2017.zip > /dev/null
!rm stuff_trainval2017.zip

!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip
!unzip annotations_trainval2017.zip > /dev/null
!rm annotations_trainval2017.zip

# clean up folder structure
! rm -r sample_data/ # remove sample_data in colab
! rm annotations/captions*.json
! rm annotations/person_keypoints*.json
! mv stuff_*.json annotations/
! mkdir images
! mv train2017 images/train
! mv val2017 images/val

## Merge annotations from COCO and COCOstuff, i.e., stuff and things
# merge training annotations
with open("annotations/instances_train2017.json") as f:
    train = json.load(f, object_pairs_hook=OrderedDict)

with open("annotations/stuff_train2017.json") as f:
    stuff_train = json.load(f, object_pairs_hook=OrderedDict)

train["categories"].extend(stuff_train["categories"])
train["annotations"].extend(stuff_train["annotations"])
assert len(train["categories"]) == 172,  "should have 172 categories after merge"

with open("annotations/train.json", "w") as f:
  json.dump(train, f)

# merge validation annotations
with open("annotations/instances_val2017.json") as f:
    val = json.load(f, object_pairs_hook=OrderedDict)

with open("annotations/stuff_val2017.json") as f:
    stuff_val = json.load(f, object_pairs_hook=OrderedDict)

val["categories"].extend(stuff_val["categories"])
val["annotations"].extend(stuff_val["annotations"])
assert len(val["categories"]) == 172,  "should have 172 categories after merge"

with open("annotations/val.json", "w") as f:
  json.dump(val, f)

from detectron2.data.datasets import register_coco_instances

register_coco_instances("COCOstuff_train", {}, "annotations/train.json", "images/train")
register_coco_instances("COCOstuff_val", {}, "annotations/val.json", "images/val")

image = cv2.imread(datapath + "/67.jpg")
cv2_imshow(image)

image = cv2.imread("images/val/000000000139.jpg")
cv2_imshow(image)

image = cv2.imread("images/train/000000000009.jpg")
cv2_imshow(image)

cfg = detectron2.model_zoo.get_config("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml", trained=True)
cfg.DATASETS.TRAIN = ("COCOstuff_train",)
cfg.DATASETS.TEST = ("COCOstuff_val",)

cfg.DATALOADER.NUM_WORKERS = 2
cfg.SOLVER.IMS_PER_BATCH = 2
cfg.SOLVER.BASE_LR = 0.00025
cfg.SOLVER.MAX_ITER = 6000 # Note that when traininig is resumed the iteration count will resume as well, so increase the number of iterations to train further. 
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128

cfg.MODEL.ROI_HEADS.NUM_CLASSES = 172

os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)

trainer = DefaultTrainer(cfg)
trainer.resume_or_load(resume=True)  # set to False to start from pretrained backbone
trainer.train()

evaluator = detectron2.evaluation.COCOEvaluator("COCOstuff_val", ("bbox",), False, output_dir="./output/")

cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.0001
trainer.test(cfg, trainer.model, evaluators=[evaluator])

# Commented out IPython magic to ensure Python compatibility.
# Look at training curves in tensorboard:
# %load_ext tensorboard
# %tensorboard --logdir output

cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.8  # set threshold for this model to visualize confident predictions only
predictor = DefaultPredictor(cfg)
outputs = predictor(image)

# visualize predictions
v = Visualizer(image[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
cv2_imshow(out.get_image()[:, :, ::-1])

!zip -r /content/ouput_2020_11_15.zip /content/output

# visualize predictions
v = Visualizer(image[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
cv2_imshow(out.get_image()[:, :, ::-1])

cfg

model.roi_heads

model

cfg.MODEL.BACKBONE.FREEZE_AT